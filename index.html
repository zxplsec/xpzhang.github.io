<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="shortcut icon" href="myIcon.ico">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

<meta name="keywords" content="Jianfeng Zhang, zhangjianfeng, jianfeng zhang, Zhang Jianfeng, Jianfeng, math, whu, Wuhan University">
<meta name="description" content="Jianfeng Zhang's home page">
<meta name="google-site-verification" content="yy_3iiS_X6pJdegdwitJMrH0LRLHXwpjrV9RKLXxKjg" />
<meta name="google-site-verification" content="yy_3iiS_X6pJdegdwitJMrH0LRLHXwpjrV9RKLXxKjg" />
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Jianfeng Zhang, Wuhan University</title>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-87320911-1', 'auto');
  ga('send', 'pageview');
</script>
</head>
<body>
<div id="layout-content" style="margin-top:25px">
 <a href="https://github.com/jfzhang95" class="github-corner"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#FD6C6C; color:#fff; position: absolute; top: 0; border: 0; right: 0;"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">
					<h1>Jianfeng Zhang (张健锋)</h1><h1>
				</h1></div>

				<h3>Undergraduate Student</h3>
				<p>
					Rm 316, Northwest Building, <br>
					Department of Applied Mathematics, <br>
					Wuhan University, <br>
					Wuhan, China. <br>
					<br>
					Email: jfzhang at whu dot edu dot cn
				</p>
                <p>
					<a href="http://www.github.com/jfzhang95"><img src="./assets_files/github_s.jpg" height="30px" style="margin-bottom:-3px"></a>
					<a href="http://www.linkedin.com/in/jianfeng-jeff-zhang-518815104"><img src="./assets_files/LinkedIn_s.png" height="30px" style="margin-bottom:-3px"></a>
				</p>
			</td>
			<td>
				<img src="./assets_files/me.jpg" border="0" width="180"><br>
			</td>
		</tr><tr>
	</tr></tbody>
</table>



<h2><font face="Arial"> Biography [<a target="_blank"
                            href="./Jianfeng_CV.pdf">CV</a>]</font></h2>
   <p>
        I am currently a research intern at OrionStar Computer Vision Lab, focusing on instance segmentation
	    and object pose estimation. Previously, I received the B.S. degree from Department of Applied Mathematics
	    in Wuhan University, supervised by <a href="http://xpzhang.me" target="_blank"> Prof. Xiaoping Zhang</a>
	    for undergraduate research. During my undergrad study, I was fortunate to be an intern at Farsee2,
	    working with <a href="https://cse.sc.edu/~songwang/" target="_blank"> Prof. Song Wang</a> and
        <a href="http://people.math.sc.edu/ju/" target="_blank"> Prof. Lili Ju</a> for Semantic Segmentation and Stereo Matching.
    </p>

    <p>
	    My research interests include 2D/3D Vision, Deep Learning and their applications.
        Recently, I am focusing on semantic segmentation on 2D/3D images, the combination of semantic
        segmentation and stereo matching, and video classification.
    </p>

    <p>
        <b>Currently applying for Ph.D position. Also open to one-year research position. If you're interested,
            don’t hesitate to contact me.</b>
    </p>



<h2><font face="Arial"> Publications </font></h2>
<table id="tbPublications" width="100%">
	<tbody>
	<tr>
		<td width="306">
		<img src="./assets_files/segresult.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Jianfeng Zhang</b>, Liezhuo Zhang, Yuankai Teng, Xiaoping Zhang, Song Wang, Lili Ju.
            "Interactive Binary Image Segmentation with Edge Preservation", submitted to
            <em>AAAI Conference on Artificial Intelligence</em> (<i><b>AAAI</b></i>), Sep. 2018.
		<p></p>
            <p>[<a href="https://arxiv.org/abs/1809.03334" target="_blank">paper</a>]</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>

    <tr>
		<td width="306">
		<img src="./assets_files/jh_graphabstract.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Jianfeng Zhang</b>, Yan Zhu, Xiaoping Zhang, Ming Ye, Jinzhong Yang.
            "Developing a Long Short-Term Memory (LSTM) based Model for Predicting Water Table Depth in Agricultural Areas",
            <em>Journal of Hydrology</em> (<i><b>JH, IF:3.73</b></i>), 2018.
		<p></p>
            <p>[<a href="./publications/JH2018.pdf" target="_blank">paper</a>] [<a href="https://github.com/jfzhang95/LSTM-water-table-depth-prediction">code</a>]</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>

    </tbody>
</table>


<h2><font face="Arial"> Projects </font></h2>
<table id="Projects" width="100%">
	<tbody>
	<tr>
		<td width="306">
		<img src="./assets_files/deeplab_result.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
        <td>
            <b>Deeplab V3+ in PyTorch</b>
        <p></p>
			We reimplement <b>Deeplab V3+</b> in PyTorch, and evaluate it on Pascal VOC 2012 and Cityscapes datasets.
			We apply Deeplab V3+ to extract the expected object from multi-view images for stereo matching,
            in order to get better 3D reconstruction results.
        <p></p>
            <p>[<a href="https://arxiv.org/pdf/1802.02611.pdf" target="_blank">paper</a>][<a href="https://github.com/jfzhang95/pytorch-deeplab-xception">code</a>]</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>

	<tr>
		<td width="306">
		<img src="./assets_files/dgc_demo.gif" height="220px" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
        <td>
            <b>Deep Grabcut (DeepGC)</b>
        <p></p>
			We replicate "<b>Deep Grabcut for Object Selection</b>", this paper proposes
						a segmentation approach that uses a rectangle as a soft constraint by
						transforming it into an Euclidean distance map. A convolutional
						encoder-decoder network is trained end-to-end by concatenating images
						with these distance maps as inputs and predicting the object masks as outputs.
						We implement this paper using PyTorch and train this model using
						Pascal VOC 2012 and SBD datasets.
        <p></p>
            <p>[<a href="https://arxiv.org/abs/1707.00243" target="_blank">paper</a>][<a href="https://github.com/jfzhang95/DeepGrabCut-PyTorch">code</a>]</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>

    <tr>
		<td width="306">
		<img src="./assets_files/demo2.gif" height="180px" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
        <td>
            <b>Video Activity Recognition in PyTorch</b>
        <p></p>
            We reimplement several video activity recognition models including <b>C3D</b>, <b>R2Plus1D</b>, <b>R3D</b>
            in PyTorch, and evaluate these models on HMDB51 and UCF101 datasets. We also apply C3D model into an online web
			game demo called "<b>You Perform, I Guess!</b>". This demo obtains <b>Excellent Demo Award</b> in DeeCamp 2018.

        <p></p>
            <p>[<a href="https://github.com/jfzhang95/pytorch-video-recognition">code</a>]
                [<a href="https://github.com/jfzhang95/project-demo">demo</a>]</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>

	<tr>
		<td width="306">
		<img src="./assets_files/rgbd_cut.png" height="200" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
        <td>
            <b>Interactive Segmentation on RGBD Image</b>
        <p></p>
            We replicate “<b>Interactive Segmentation on RGBD Images via Cue Selection</b>”,
			this paper proposes a novel interactive segmentation algorithm which can incorporate
			multiple feature cues like color, depth and normals in a graph cut framework.
        <p></p>
            <p>[<a href="http://openaccess.thecvf.com/content_cvpr_2016/papers/Feng_Interactive_Segmentation_on_CVPR_2016_paper.pdf" target="_blank">paper</a>]
				[<a href="https://github.com/ZVsion/rgbd_image_segmentation">code</a>]</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>

	<tr>
		<td width="306">
		<img src="./assets_files/tv_result.png" height="200" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
        <td>
            <b>Total Variation Image Segmentation Model Based on Primal-dual Method</b>
        <p></p>
			We implement total variation image segmentation algorithm and
			applied primal-dual method to optimize it. In order to obtain better results, we
			integrate user provided information into algorithm. In addition, we also
			combine the model with K-means to improve the final results.
        <p></p>
            <p>[<a href="https://github.com/jfzhang95/TV_Segmentation/blob/master/notes.pdf" target="_blank">pdf</a>]
				[<a href="https://github.com/jfzhang95/TV_Segmentation">code</a>]</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>

	<tr>
		<td width="306">
		<img src="./assets_files/videoseg_result.gif" height="240px" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
        <td>
            <b>Video Object Segmentation Framework</b>
        <p></p>
			We build a Video Object Segmentation Framework. In this framework, we use MobileNetV2 to extract each
			video frame's feature and utilize a very simple algorithm to determine if a certain frame is key frame.
			Based on aformentioned method, we can extract expected number of key frames (usually set to 5~15 frames). After get
			video key frames, we utilize YOLOV2 and Deep Grabcut to detect center object and obtain object mask, respectively.
			Finally, we use key frames masks to finetune segmentation network before segment the whole video. Only 200~3000
			iters of online fine-tune can reach very good video object segmentation results. This framework is now incorporate
			into our company's product and applied to extract products in E-commerce videos and people in videos from
            Douyin, Kuaishou and YouTube.
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>

	<tr>
		<td width="306">
		<img src="./assets_files/ali_result.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
        <td>
            <b>End to End Network Image Text Detection and Recognition</b>
        <p></p>
			We build a faster-rcnn like model for end to end network image text detection and recognition.
						Our method achieves 0.634 accuracy and ranks <b>13/1488</b> on
            <a target="_blank" href="https://tianchi.aliyun.com/competition/introduction.htm?spm=5176.11165320.5678.1.2e31572eYD2QEf&raceId=231652">ICPR MTWI 2018 Challenge on Tianchi</a>.
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>

	<tr>
		<td width="306">
		<img src="./assets_files/jd_result.png" height="200" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
        <td>
            <b>Clothing Fashion Style Recognition</b>
        <p></p>
			We build our fashion style recognition model based on ResNet, DenseNet and VGG. We
                        also use Deeplab V3+ to extract clothes and remove background, which could help to denoise.
                        In addition, we apply model ensemble method to improve our results.
						Finally, our method achieves 0.610 F2 score and ranks <b>9/213</b> on <a target="_blank" href="https://fashion-challenge.github.io/#index">JD AI Fashion-Challenge</a>.
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>

    </tbody>
</table>

<h2>Honors &amp; Awards</h2>
<table style="border-spacing:2px">
    <tbody>
        <tr><td> Excellent Demo Award, DeeCamp, 2018</td></tr>
		<tr><td> Best Bachelor Thesis Award (<b>1/180+</b>, School of Mathematics and Stastics, Wuhan University), 2018</td></tr>
		<tr><td> Award for Merit Student of Wuhan University, 2015-2017</td></tr>
		<tr><td> Honorable Mention, The Mathematical Contest in Modeling (MCM), Consortium for Mathematics and Its Application, 2015</td></tr>
	</tbody>
</table>



<h2>Teaching</h2>
<table id="tbTeaching" border="0" width="100%">
	<tbody>
		<tr>
			<td> 2017-2018</td><td>Spring</td><td>C Language Programming</td>
		</tr>
		<tr>
			<td> 2017-2018</td><td>Fall</td><td>Data Structures and Algorithms in Python</td>
		</tr>
	</tbody>
</table>



<div id="footer">
	<div id="footer-text"></div>
</div>
	<p><center><font face="Arial">
        <br>
            <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=360&t=tt&d=notuMmjZmmmqgGr27ELZx5OsJzDyX5EM5xhYhsafATk&co=2d78ad&ct=ffffff&cmo=3acc3a&cmn=ff5353'></script>
            &copy; Jianfeng Zhang | Last updated: 09/12/2018
        </font></center></p>

